{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "Here, a generative adversarial network (GAN) is trained on the Fashion-MNIST dataset. Objective of the GAN is to generate new fashion clothes.\n",
    "GANs were developed by Ian Goodfellow et al. in 2014 ([see](https://arxiv.org/abs/1406.2661)).\n",
    "GANs relies on the idea to have two networks, a generator $G$ and a discrimininator $D$, competing agiant each other. While $G$ tries to create fake data, the discriminator task is to determine whether the data passed to it is drawn from real data or it is generated by $G$. It is, in fact, similar to a two-party game, in which $G$ and $D$ are improving their performance throguh competition between each other. The value function for GAN can be written as,\n",
    "\n",
    "$\\min \\max V(D, G) = \\mathrm{E}_{x\\sim p_{data(x)}} [\\log D(x)] + \\mathrm{E}_{z\\sim p_z(z)} [ \\log (1-D(G(z))]$\n",
    "\n",
    "The idea behind GANs is that you have two networks, a generator $G$ and a discriminator $D$, competing against each other. The generator makes fake data to pass to the discriminator. The discriminator also sees real data and predicts if the data it's received is real or fake. The generator is trained to fool the discriminator, it wants to output data that looks _as close as possible_ to real data. And the discriminator is trained to figure out which data is real and which is fake. What ends up happening is that the generator learns to make data that is indistiguishable from real data to the discriminator.\n",
    "\n",
    "![GAN diagram](assets/gan_diagram.png)\n",
    "\n",
    "The general structure of a GAN is shown in the diagram above, using Fashion-MNIST images as data. The latent sample is a random vector the generator uses to contruct it's fake images. As the generator learns through training, it figures out how to map these random vectors to recognizable images that can foold the discriminator.\n",
    "\n",
    "The output of the discriminator is a sigmoid function, where 0 indicates a fake image and 1 indicates an real image. If you're interested only in generating new images, you can throw out the discriminator after training. The code is written in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Fashion_MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting Fashion_MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting Fashion_MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting Fashion_MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('Fashion_MNIST_data')\n",
    "import mnist_reader\n",
    "X_train, y_train = mnist_reader.load_mnist('Fashion_MNIST_data', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('Fashion_MNIST_data', kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Visulaization\n",
    "\n",
    "See Fashion dataset samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d898060080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nypz+geBZ6S0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d8927e4c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(1, 1))\n",
    "img = X_train[300]\n",
    "\n",
    "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  GAN Inputs\n",
    "\n",
    "Two sets of input should be created for the network. Inputs of the generator, which are random, are called inputs_z. Inputs of discriminator, which are fashion-MNIST data set, are called inputs_real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, real_dim), name='input_real') \n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    \n",
    "    return inputs_real, inputs_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GAN Network \n",
    "\n",
    "The GAN network is shown in figure below.\n",
    "\n",
    "![GAN Network](assets/gan_network.png)\n",
    "\n",
    "\n",
    "## Generator Network\n",
    "The generator network is created here with two hidden layers using leaky ReLU to aviod gradient vanishing and in order to make sure that it has enough capacity for function approximation. \n",
    "\n",
    "### TensorFlow Variable Scope\n",
    "\n",
    "During training, the generator and discriminator are optimized seperately. Furthermore, it is possible to use the reuse keyword in order to sample during training or after training. \n",
    "\n",
    "\n",
    "### Action Function\n",
    "#### Leaky ReLU\n",
    "The leaky ReLU is defined as follow, \n",
    "\n",
    "$$\n",
    "f(x) = max(\\alpha \\times x, x)\n",
    "$$\n",
    "where $x$ is the input. If it is positive, the ouput is $x$ , otherwise, it is $\\alpha \\times x$.\n",
    "\n",
    "#### Tanh \n",
    "According to previous studies, the generator has the best performance with $tanh$ for the its output.\n",
    "\n",
    "## Discriminator Network\n",
    "\n",
    "The only difference between the discriminator and generator networks is the ouput layer. The discriminator uses sigmoid activation instead of $tanh$.\n",
    "#### Sigmoid\n",
    "The Sigmoid function is defined as,\n",
    "$$\n",
    "    \\sigma (x) = \\frac{1}{1+\\exp(-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, out_dim, n_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(z, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(alpha * h1, h1)\n",
    "        h2 = tf.layers.dense(h1, n_units * 2 , activation=None)\n",
    "        h2 = tf.maximum(alpha * h2, h2)\n",
    "        # Logits and tanh output\n",
    "        logits = tf.layers.dense(h2, out_dim, activation=None)\n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(x, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(alpha * h1, h1)\n",
    "        h2 = tf.layers.dense(h1, n_units / 2, activation=None)\n",
    "        h2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "        logits = tf.layers.dense(h2, 1, activation=None)\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of input image to discriminator\n",
    "input_size = 784\n",
    "# Size of latent vector to generator\n",
    "z_size = 128\n",
    "# Sizes of hidden layers in generator and discriminator\n",
    "g_hidden_size = 256\n",
    "d_hidden_size = 256\n",
    "# Leak factor for leaky ReLU\n",
    "alpha = 0.01\n",
    "# Smoothing \n",
    "smooth = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "\n",
    "The networks for both $G$ and $D$ are created here. Furthermore, two networks are built for $D$, one for real input and one for generated data by $G$. As the weights should be the same for both real and generated inputs,the reuse keyword is used. For the generated data, it is ouput of $G$, here defined as `g_model`. \n",
    "Therefore, the real input of the discriminator is `discriminator(input_real)` while the fake discriminator is `discriminator(g_model, reuse=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-166e56343b42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#  Input placeholders are created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minput_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# The generator network is created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_size' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#  Input placeholders are created\n",
    "input_real, input_z = model_inputs(input_size, z_size)\n",
    "\n",
    "# The generator network is created\n",
    "g_model = generator(input_z, input_size, n_units=g_hidden_size, alpha=alpha)\n",
    "\n",
    "# The discriminator networks are created one for real input and one for generated input\n",
    "d_model_real, d_logits_real = discriminator(input_real, n_units=d_hidden_size, alpha=alpha)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $D$ and $G$ Losses\n",
    "\n",
    "Now we need to calculate the losses, which is a little tricky. For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_loss_real + d_loss_fake`. The losses will by sigmoid cross-entropys, which we can get with `tf.nn.sigmoid_cross_entropy_with_logits`. We'll also wrap that in `tf.reduce_mean` to get the mean for all the images in the batch. So the losses will look something like \n",
    "\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "```\n",
    "\n",
    "For the real image logits, we'll use `d_logits_real` which we got from the discriminator in the cell above. For the labels, we want them to be all ones, since these are all real images. To help the discriminator generalize better, the labels are reduced a bit from 1.0 to 0.9, for example,  using the parameter `smooth`. This is known as label smoothing, typically used with classifiers to improve performance. In TensorFlow, it looks something like `labels = tf.ones_like(tensor) * (1 - smooth)`\n",
    "\n",
    "The discriminator loss for the fake data is similar. The logits are `d_logits_fake`, which we got from passing the generator output to the discriminator. These fake logits are used with labels of all zeros. Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "Finally, the generator losses are using `d_logits_fake`, the fake image logits. But, now the labels are all ones. The generator is trying to fool the discriminator, so it wants to discriminator to output ones for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "d_loss_real = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, \n",
    "                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                          labels=tf.zeros_like(d_logits_real)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                     labels=tf.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "We want to update the generator and discriminator variables separately. So we need to get the variables for each part build optimizers for the two parts. To get all the trainable variables, we use `tf.trainable_variables()`. This creates a list of all the variables we've defined in our graph.\n",
    "\n",
    "For the generator optimizer, we only want to generator variables. Our past selves were nice and used a variable scope to start all of our generator variable names with `generator`. So, we just need to iterate through the list from `tf.trainable_variables()` and keep variables to start with `generator`. Each variable object has an attribute `name` which holds the name of the variable as a string (`var.name == 'weights_0'` for instance). \n",
    "\n",
    "We can do something similar with the discriminator. All the variables in the discriminator start with `discriminator`.\n",
    "\n",
    "Then, in the optimizer we pass the variable lists to `var_list` in the `minimize` method. This tells the optimizer to only update the listed variables. Something like `tf.train.AdamOptimizer().minimize(loss, var_list=var_list)` will only train the variables in `var_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Get the trainable_variables, split into G and D parts\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 200\n",
    "samples = []\n",
    "losses = []\n",
    "# Only save generator variables\n",
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(data.train.num_examples//batch_size):\n",
    "            batch = data.train.next_batch(batch_size)\n",
    "            \n",
    "            # Get images, reshape and rescale to pass to D\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size)) # modified\n",
    "            #batch_z = batch_images#batch[0].reshape((z_size, 784)) #modified\n",
    "            #batch_z = 2 * batch_z - 1\n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})\n",
    "        \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})\n",
    "        train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))    \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                       generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha),\n",
    "                       feed_dict={input_z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "Here we'll check out the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator')\n",
    "plt.plot(losses.T[1], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator samples from training\n",
    "\n",
    "Here we can view samples of images from the generator. First we'll look at images taken while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "    \n",
    "    return fig, axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load samples from generator taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are samples from the final training epoch. You can see the generator is able to reproduce numbers like 1, 7, 3, 2. Since this is just a sample, it isn't representative of the full range of images this generator can make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm showing the generated images as the network was training, every 10 epochs. With bonus optical illusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols = 15, 8\n",
    "fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    "\n",
    "for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n",
    "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts out as all noise. Then it learns to make only the center white and the rest black. You can start to see some number like structures appear out of the noise like 1s and 9s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the generator\n",
    "\n",
    "We can also get completely new images from the generator by using the checkpoint we saved after training. We just need to pass in a new latent vector $z$ and we'll get new samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    sample_z = np.random.uniform(-1.0, 1.0, size=(16, z_size))\n",
    "    gen_samples = sess.run(\n",
    "                   generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha),\n",
    "                   feed_dict={input_z: sample_z})\n",
    "_ = view_samples(0, [gen_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
