\input lec_preamble.tex

%% YOUR LOCAL MACROS AND PACKAGE GO HERE
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\matx}{\ensuremath{\mathcal{X}}}
\def\PP{\mathbb{P}}
\newcommand{\matf}{\ensuremath{\mathcal{F}}}

%% SOME RULES
%% - only use pdfLaTeX to compile!!
%% - for aligned equations, use align, do NOT use eqnarray
%% - Use \| instead of ||, e.g., for divergence D(P\|Q), norms \|x\|_2
%% - for unreferenced displayed math, use \[\] and do not number them
%% - for labeling, use the following convention
%%          theorem: 			\label{thm:fermat}
%%          definition: 	\label{def:entropy}
%%          lemma: 				\label{lmm:fermat}
%%          proposition: 	\label{prop:fermat}, etc
%% - for bibliography use bibtex and add your entry to bibnotes.bib


%% lecture starts below
%% 1. lecture number (also name the file accordingly like lec01.tex)
%% 2. title (short summary of the content)
%% 3. scribe name (that is you)
%% 4. lecture date
\mlecture{1}{Fashion-MNIST Generative Adversarial Network }{Oct 10, 2017}


\section{Introduction }


Recent advancements in deep learning manily involve discriminative models with application in classification of high dimensional inputs. The cornerstones of these  advancements are attributed to the backpropagation and dropout methods, as well as piecewise linear units with well-behaved gradient. In 2014, generative adversarial networks (GAN) have been proposed by Ian Goodfellow to solve the problems with generative models. So far the main challenge for generative models is the complexity of approximating the numerous burdensome probabilistic computation dealing with maximum likelihood and other strategies. GAN takes advantage of the competition between two models,  discriminative model ($D$) and generative model ($G$). The role of  $D$ is to determine the source of its inputs, which can be drawn from a sample generated by $G$ or real data. Therefore, it is very similar to a two-agent game, in which generative network tries to mislead the discriminative network by generating samples similar to real data. 

\end{itemize}
\section{Adversarial Networks Theory }
The generatorâ€™s distribution $p_g$ over data $x$ can be learned with definition of a prior on input noise variables $p_z(z)$. Then, using a multilayer perceptron with parameters $\theta_g$, a mapping is defined between input noise and data space  $G(z; \theta_g))$. The second multilayer perceptron ($D(x; \theta_d)$) with parameter $\theta_d$ is chosen such that it receives $x$ and $G(z, \theta_G)$ and determines whether they are coming out of data or $p_g$. The output of $D$ is a scalar which determines the probability of images origin. Therefore, $D$ is trained in a way to maximize the probability of assigning the correct label to both training data and data drawn from $G$. While $G$ is trained such that it excellences at generation of realistic data. To do so, $G$ tries to minimize $\log(1-D(G(z))$. The following equation shows such training process,
$$
\min \max V(D,G) = \Expect_{x ~ p_{data(x)}} [\log D(x) ] + \Expect_{z~p_z(z)}[ \log(1 - D(G(z)))]
$$
Where $V(D, G)$ is value function. 

\section{Proposed Work}

In this study, GAN is going to be applied Fashion-MNIST dataset with objective to create a generator with capability to create image of fashion clothes, which include T-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. In general, MNIST dataset is so simple, and networks working on this dataset are not necessary capable to real world problems, even with accuracy of 97$\%$ during training and testing. The objective here is to test GAN in a more challenging problem. To do so, neural network is going to be created by TensorFlow by google. 

\end{section}





%end lec 12
\ifmBIGFILE\else
\bibliographystyle{alpha}
\bibliography{IEEEabrv,bibnotes}
\end{document}
\fi
